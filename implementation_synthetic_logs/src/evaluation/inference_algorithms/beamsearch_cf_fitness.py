"""
this file is build based on the code found in evaluate_suffix_and_remaining_time.py
here the beam search (with breath-first-search) is implemented, to find compliant prediction
Author: Anton Yeshchenko
"""

from __future__ import division
import csv
from pathlib import Path
from queue import PriorityQueue
import distance
import numpy as np
import pandas as pd
from tensorflow import keras
from jellyfish import damerau_levenshtein_distance

from src.commons import shared_variables as shared
from src.commons.log_utils import LogData
from src.evaluation.prepare_data import get_act_prediction, get_beam_size, encode, get_pn_fitness, get_token_fitness, get_token_fitness2,  get_pn_fitness2, get_tr_fitness
from src.training.train_common import CustomTransformer
import dask.dataframe as dd
from dask.multiprocessing import get
import absl.logging
absl.logging.set_verbosity(absl.logging.ERROR)
from dask.distributed import Client
from tqdm import tqdm


def run_experiments(log_data: LogData, compliant_traces: pd.DataFrame, maxlen, predict_size, char_indices,
                    target_char_indices, target_indices_char, model_file: Path, output_file: Path, 
                    pn_file: Path, pt_file: Path):
    # Find cycles and modify the probability functionality goes here
    stop_symbol_probability_amplifier_current = 1

    # Load model, set this to the model generated by train.py
    model = keras.models.load_model(model_file, custom_objects={'CustomTransformer': CustomTransformer})

    class NodePrediction:
        def __init__(self, crop_trace: pd.DataFrame, probability_of=0):
            self.cropped_trace = crop_trace
            self.cropped_line = ''.join(crop_trace[log_data.act_name_key].tolist())
            self.model_input = encode(self.cropped_line, maxlen, char_indices)
            self.probability_of = probability_of

        def __str__(self):
            return f"Prefix: {self.cropped_line}, prob. {self.probability_of}"

        def __lt__(self, other):
            return -self.probability_of < -other.probability_of

    class CacheFitness:
        def __init__(self):
            self.trace = {}
            
        def add(self, crop_trace:str, fitness: float):
            self.trace[crop_trace] = fitness
            
        def get(self, crop_trace:str):
            if crop_trace not in self.trace.keys():
                return None
            else:
                return self.trace[crop_trace]


    class CacheTrace:
        def __init__(self):
            self.trace = {}
            
        def add(self, crop_trace:str, output: list):
            self.trace[crop_trace] = output
            
        def get(self, crop_trace:str):
            if crop_trace not in self.trace.keys():
                return None
            else:
                return self.trace[crop_trace]





    def jh(trace, prefix_size, log_data, predict_size, pn_file, pt_file, target_indices_char, target_char_indices, weight):

        if len(trace) >= prefix_size:          
            trace_name = trace[log_data.case_name_key].tolist()[0]
            trace_prefix = trace.head(prefix_size)
            
            act_prefix = ''.join(trace_prefix[log_data.act_name_key].tolist()) + "_" + str(weight)
            
            check_prefix = cache_trace.get(act_prefix)
            if check_prefix == None:
            
                
                
                trace_ground_truth = trace.tail(trace.shape[0] - prefix_size)
                act_ground_truth = ''.join(trace_ground_truth[log_data.act_name_key].tolist())
                act_ground_truth_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in act_ground_truth]
                # Initialize queue for beam search, put root of the tree inside
                visited_nodes: PriorityQueue[NodePrediction] = PriorityQueue()
                visited_nodes.put(NodePrediction(trace_prefix))

                frontier_nodes: PriorityQueue[NodePrediction] = PriorityQueue()
                found_satisfying_constraint = False

                child_node = None
                for i in range(predict_size - prefix_size + 1):
                    # if len( [i for i in act_ground_truth_org if i in ['Unexpected2', 'Repairing2', 'Unexpected1', 'Repairing1'] ] ) > 0:
                    #     print("prefix_step: ", i)
                    if visited_nodes.empty():
                            break
                    
                    for k in range(min(shared.beam_size, len(visited_nodes.queue))):                        
                        child_node = visited_nodes.get()

                        if child_node.cropped_line[-1] == "!":
                            # break
                            if k == 0:
                                visited_nodes = PriorityQueue()
                                break
                            else:
                                continue
                        
                        
                        # print("prefix_step = ", i, " , beam_step = ", k)  
                        enc = child_node.model_input
                        temp_cropped_trace = child_node.cropped_trace
                        temp_cropped_line = child_node.cropped_line
                        y = model.predict(enc, verbose=0)  # make predictions
                        # split predictions into separate activity and outcome predictions
                        y_char = y[0]
                        y_char = np.append(y_char, [min(y_char)]*(len(target_indices_char)- len(y_char)))
                        # Combine with fitness
                        fitness = [] 
                        for f in range(len(target_indices_char) + len(log_data.new_chars)):
                            
                            if f < len(target_indices_char):
                                temp_prediction = target_indices_char[f]               
                            else:
                                temp_prediction = log_data.new_chars[f-len(target_indices_char)]
                            
                            temp_prediction = target_indices_char[f]
                            predicted_row = temp_cropped_trace.tail(1).copy()
                            predicted_row.loc[:, log_data.act_name_key] = temp_prediction
                            temp_cropped_trace_next= pd.concat([temp_cropped_trace, predicted_row])   
                            temp_cropped_line_next = ''.join(temp_cropped_trace_next[log_data.act_name_key].tolist())       
                            fitness_current =  get_token_fitness2(pn_file, temp_cropped_trace_next, log_data)[trace_name]        
                            fitness = fitness + [np.exp(fitness_current) ]
                            
                            
                            check_cache = cache_fitness.get(temp_cropped_line_next )
                            if check_cache == None:
                                fitness_current = get_token_fitness2(pn_file, temp_cropped_trace_next, log_data)[trace_name] 
                                cache_fitness.add(temp_cropped_line_next, fitness_current)
                            else:
                                fitness_current = check_cache
                        
                        if sum(fitness) > 0:
                            fitness = [f/sum(fitness) for f in fitness] 
                        else:
                            fitness = np.repeat(1/len(fitness),len(fitness)).tolist()  
                        
                        

                        
                        
                        # tmp_prefix_probability = child_node.probability_of + np.log(y_char)
                        # score = tmp_prefix_probability
                        # score = [(1-weight)*a + weight*b for a,b in zip(tmp_prefix_probability, np.log(fitness))]
                        
                        score = [pow(a,1-weight)*pow(b,weight) for a,b in zip(y_char, fitness)]
                        
                        # put top 3 based on score
                        frontier_nodes = get_beam_size(frontier_nodes, NodePrediction, child_node, 
                                                    temp_cropped_line, temp_cropped_trace, score, y_char,fitness, act_ground_truth_org,
                                                    target_indices_char, target_char_indices, 
                                                    log_data, beam_size = shared.beam_size)
                        
                    visited_nodes = frontier_nodes
                    frontier_nodes = PriorityQueue()

                predicted = child_node.cropped_line[prefix_size:-1]

                predicted_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in predicted]
                act_ground_truth_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in act_ground_truth]
                # print("prediction: ", predicted_org)
                # print("groundtruth: ", act_ground_truth_org)
                
                output = []
                if len(act_ground_truth) > 0:
                    output.append(trace_name)

                    output.append(prefix_size)
                    output.append(act_ground_truth)
                    output.append(predicted)

                    dls = 1 - \
                        (damerau_levenshtein_distance(predicted, act_ground_truth) / max(len(predicted), len(act_ground_truth)))
                    if dls < 0:
                        dls = 0
                    output.append(dls)
                    output.append(1 - distance.jaccard(predicted, act_ground_truth))
                    

                    output.append(weight)
                    output.append('>>'.join(act_ground_truth_org))
                    
                    prediction_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in predicted]
                    output.append('>>'.join(prediction_org))
                    
                    # output.append('1' if compliantness else '0')
                    cache_trace.add(act_prefix, output)
                    
            else:
                trace_ground_truth = trace.tail(trace.shape[0] - prefix_size)
                act_ground_truth = ''.join(trace_ground_truth[log_data.act_name_key].tolist())
                output = []
                
                output.append(trace_name)
                output.append(prefix_size)
                output.append(act_ground_truth)
                
                predicted = check_prefix[3]
                output.append(predicted)
                
                dls = 1 - \
                    (damerau_levenshtein_distance(predicted, act_ground_truth) / max(len(predicted), len(act_ground_truth)))
                if dls < 0:
                    dls = 0
                output.append(dls)
                output.append(1 - distance.jaccard(predicted, act_ground_truth))
                output = output + check_prefix[6:(len(check_prefix)-2)]
                
                act_ground_truth_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in act_ground_truth]
                output.append('>>'.join(act_ground_truth_org))
                
                prediction_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in predicted]
                output.append('>>'.join(prediction_org))   

            if output:
                with open(output_file, 'a', encoding='utf-8', newline='') as csvfile:
                    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
                    spamwriter.writerow(output)


##############################################################

    with open(output_file, 'w', encoding='utf-8', newline='') as csvfile:
        spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
        # Headers for the new file
        spamwriter.writerow(["Case ID", "Variant ID", "Variant_size", "Variant_size_train", "Variant_size_test", "Prefix length", "Ground truth", "Predicted", "Damerau-Levenshtein", "Jaccard",
                                "Weight", "Record_org", "Prediction_org"])

    cache_fitness = CacheFitness()
    cache_trace = CacheTrace()

    for prefix_size in range(log_data.evaluation_prefix_start, log_data.evaluation_prefix_end+1):
        compliant_traces = compliant_traces.reset_index(drop=True) 
        print(prefix_size)
        for weight in [round(0.1*i, 1) for i in range(5)] + [0.5 + round(0.05*i, 2) for i in range(10)]: # [0.92, 0.94, 0.96, 0.98, 1.0]:[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9, 1.00]
            tqdm.pandas()
            compliant_traces.groupby(log_data.case_name_key).progress_apply(lambda x: jh(x, prefix_size, log_data,
                                                                                    predict_size, pn_file, pt_file,
                                                                                    target_indices_char,
                                                                                    target_char_indices, weight))
