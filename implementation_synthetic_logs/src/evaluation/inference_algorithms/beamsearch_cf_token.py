"""
this file is build based on the code found in evaluate_suffix_and_remaining_time.py
here the beam search (with breath-first-search) is implemented, to find compliant prediction
Author: Anton Yeshchenko
"""

from __future__ import division
import csv
from pathlib import Path
from queue import PriorityQueue
import distance
import numpy as np
import pandas as pd
from tensorflow import keras
from jellyfish import damerau_levenshtein_distance

from src.commons import shared_variables as shared
from src.commons.log_utils import LogData
from src.evaluation.prepare_data import get_act_prediction, get_beam_size, encode, get_token_fitness, get_tr_fitness, get_token_fitness2
from src.training.train_common import CustomTransformer
import absl.logging
absl.logging.set_verbosity(absl.logging.ERROR)
from tqdm import tqdm


def run_experiments(log_data: LogData, compliant_traces: pd.DataFrame, maxlen, predict_size, char_indices,
                    target_char_indices, target_indices_char, model_file: Path, output_file: Path, 
                    pn_file: Path, pt_file: Path):
    # Find cycles and modify the probability functionality goes here
    stop_symbol_probability_amplifier_current = 1

    # Load model, set this to the model generated by train.py
    model = keras.models.load_model(model_file, custom_objects={'CustomTransformer': CustomTransformer})

    class NodePrediction:
        def __init__(self, crop_trace: pd.DataFrame, probability_of=0):
            self.cropped_trace = crop_trace
            self.cropped_line = ''.join(crop_trace[log_data.act_name_key].tolist())
            self.model_input = encode(self.cropped_line, maxlen, char_indices)
            self.probability_of = probability_of

        def __str__(self):
            return f"Prefix: {self.cropped_line}, prob. {self.probability_of}"

        def __lt__(self, other):
            return -self.probability_of < -other.probability_of

    def jh(trace, prefix_size, log_data, predict_size, pn_file, pt_file, target_indices_char, target_char_indices, thre):

        if len(trace) >= prefix_size:          
            
            trace_name = trace[log_data.case_name_key].tolist()[0]
            trace_prefix = trace.head(prefix_size)
            trace_ground_truth = trace.tail(trace.shape[0] - prefix_size)
            act_ground_truth = ''.join(trace_ground_truth[log_data.act_name_key].tolist())
            outcome_ground_truth = trace[log_data.label_name_key].iloc[0]

            # Initialize queue for beam search, put root of the tree inside
            visited_nodes: PriorityQueue[NodePrediction] = PriorityQueue()
            visited_nodes.put(NodePrediction(trace_prefix))

            frontier_nodes: PriorityQueue[NodePrediction] = PriorityQueue()
            found_satisfying_constraint = False

            child_node = None
            for i in range(predict_size - prefix_size +1):
                if visited_nodes.empty():
                        break
        
                for k in range(min(shared.beam_size, len(visited_nodes.queue))):                        
                    child_node = visited_nodes.get()

                    if child_node.cropped_line[-1] == "!":
                        # break
                        if k == 0:
                            visited_nodes = PriorityQueue()
                            break
                        else:
                            continue

                    enc = child_node.model_input
                    temp_cropped_trace = child_node.cropped_trace
                    temp_cropped_line = child_node.cropped_line
                    y = model.predict(enc, verbose=0)  # make predictions
                    # split predictions into separate activity and outcome predictions
                    y_char = y[0][0]
                    y_o = y[1][0][0]

                    # Combine with fitness
                    fitness = [] 
                    for f in range(len(y_char)):
                        temp_prediction = target_indices_char[f]
                        predicted_row = temp_cropped_trace.tail(1).copy()
                        predicted_row.loc[:, log_data.act_name_key] = temp_prediction
                        temp_cropped_trace_next= pd.concat([temp_cropped_trace, predicted_row])                        
                        fitness = fitness +  [get_token_fitness2(pn_file, temp_cropped_trace_next, log_data)[trace_name] ]
                        
                    score = [a*b for a,b in zip(y_char, fitness)]
                                     
                    # put top 3 based on score
                    frontier_nodes = get_beam_size(frontier_nodes, NodePrediction, child_node, 
                                                   temp_cropped_line, temp_cropped_trace, score, 
                                                   target_indices_char, target_char_indices, 
                                                   log_data, beam_size = shared.beam_size)
                    
                visited_nodes = frontier_nodes
                frontier_nodes = PriorityQueue()

            predicted = child_node.cropped_line[prefix_size:-1]
            predicted_outcome = '1' if y_o >= 0.5 else '0'

            
            output = []
            if len(act_ground_truth) > 0:
                output.append(prefix_size)
                output.append(act_ground_truth)
                output.append(predicted)

                dls = 1 - \
                    (damerau_levenshtein_distance(predicted, act_ground_truth) / max(len(predicted), len(act_ground_truth)))
                if dls < 0:
                    dls = 0
                output.append(dls)
                output.append(1 - distance.jaccard(predicted, act_ground_truth))

                output.append(outcome_ground_truth)
                output.append(predicted_outcome)
                output.append('1' if outcome_ground_truth == predicted_outcome else '0')

                output.append(thre)
                # output.append('1' if compliantness else '0')

            if output:
                with open(output_file, 'a', encoding='utf-8', newline='') as csvfile:
                    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
                    spamwriter.writerow(output)

##############################################################

    with open(output_file, 'w', encoding='utf-8', newline='') as csvfile:
        spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
        # Headers for the new file
        spamwriter.writerow(["Prefix length", "Ground truth", "Predicted", "Damerau-Levenshtein", "Jaccard",
                             "Ground truth outcome", "Predicted outcome", "Outcome diff.", "threshold"])


    for prefix_size in range(log_data.evaluation_prefix_start, log_data.evaluation_prefix_end+1):
        compliant_traces = compliant_traces.reset_index(drop=True) 

        for thre in [1.0]: # [0.92, 0.94, 0.96, 0.98, 1.0]:
            tqdm.pandas()
            compliant_traces.groupby(log_data.case_name_key).progress_apply(lambda x: jh(x, prefix_size, log_data,
                                                                                    predict_size, pn_file, pt_file,
                                                                                    target_indices_char,
                                                                                    target_char_indices, thre))
