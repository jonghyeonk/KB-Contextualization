"""
this script takes as input the LSTM or RNN weights found by train.py
change the path in the shared_variables.py to point to the h5 file
with LSTM or RNN weights generated by train.py
Author: Niek Tax
"""

from __future__ import division
import csv
from pathlib import Path

from datetime import timedelta #jh
import numpy as np #jh
# from sklearn import metrics #jh
# from evaluation.prepare_data import select_petrinet_verified_traces, prepare_testing_data

import pandas as pd
from jellyfish import damerau_levenshtein_distance
import distance
from tensorflow import keras

from src.commons.log_utils import LogData
from src.commons.utils import extract_trace_sequences
from src.evaluation.prepare_data import encode, get_act_prediction
from src.training.train_common import CustomTransformer


def run_experiments(log_data: LogData, compliant_traces: pd.DataFrame, maxlen, predict_size, char_indices,
                    target_char_indices, target_indices_char, model_file: Path, output_file: Path):
    # Load model, set this to the model generated by train.py
    model = keras.models.load_model(model_file, custom_objects={'CustomTransformer': CustomTransformer})

    with open(output_file, 'w', newline='') as csvfile:
        spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
        spamwriter.writerow(["Prefix length", "Ground truth", "Predicted", "Damerau-Levenshtein", "Jaccard",
                             "Ground truth outcome", "Predicted outcome", "Outcome diff."])

    for prefix_size in range(log_data.evaluation_prefix_start, log_data.evaluation_prefix_end+1):
        print("Prefix size: " + str(prefix_size))

        for trace_name, trace in compliant_traces.groupby(log_data.case_name_key):
            lines, _, outcomes = extract_trace_sequences(log_data, [trace_name])
            line = lines[0]
            outcome = outcomes[0]

            if len(line) < prefix_size:
                continue  # Make no prediction for this case, since this case has ended already

            cropped_line = ''.join(line[: prefix_size])
            ground_truth = ''.join(line[prefix_size: prefix_size+predict_size])
            ground_truth_o = outcome
            predicted = ''
            predicted_outcome = ''

            for i in range(predict_size - prefix_size):
                enc = encode(cropped_line, maxlen, char_indices)
                y = model.predict(enc, verbose=0)  # make predictions
                # split predictions into separate activity and outcome predictions
                y_char = y[0][0]
                y_o = y[1][0][0]

                # undo one-hot encoding
                prediction = get_act_prediction(cropped_line, y_char, target_indices_char, target_char_indices)
                predicted_outcome = '1' if y_o >= 0.5 else '0'

                if prediction == '!':
                    # end of case was just predicted, therefore, stop predicting further into the future
                    break

                cropped_line += prediction  # add the prediction to the suffix
                predicted += prediction

            output = []
            if len(ground_truth) > 0:
                output.append(prefix_size)
                output.append(ground_truth)
                output.append(predicted)
                dls = 1 - \
                    (damerau_levenshtein_distance(predicted, ground_truth) / max(len(predicted), len(ground_truth)))
                if dls < 0:
                    dls = 0
                output.append(dls)
                output.append(1 - distance.jaccard(predicted, ground_truth))

                output.append(ground_truth_o)
                output.append(predicted_outcome)
                output.append('1' if ground_truth_o == predicted_outcome else '0')

            if output:
                with open(output_file, 'a', newline='') as csvfile:
                    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
                    spamwriter.writerow(output)
