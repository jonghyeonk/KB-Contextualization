"""
this file is build based on the code found in evaluate_suffix_and_remaining_time.py
here the beam search (with breath-first-search) is implemented, to find compliant prediction
Author: Anton Yeshchenko
"""

from __future__ import division
import csv
from pathlib import Path
from queue import PriorityQueue
import distance
import numpy as np
import pandas as pd
from tensorflow import keras
from jellyfish import damerau_levenshtein_distance

from src.commons import shared_variables as shared
from src.commons.log_utils import LogData
from src.evaluation.prepare_data import get_act_prediction, get_beam_size, encode, get_token_fitness, get_tr_fitness
from src.training.train_common import CustomTransformer
import dask.dataframe as dd
from dask.multiprocessing import get
import absl.logging
absl.logging.set_verbosity(absl.logging.ERROR)
from dask.distributed import Client
from tqdm import tqdm


def run_experiments(log_data: LogData, compliant_traces: pd.DataFrame, maxlen, predict_size, char_indices,
                    target_char_indices, target_indices_char, model_file: Path, output_file: Path, 
                    pn_file: Path, pt_file: Path):
    # Find cycles and modify the probability functionality goes here
    stop_symbol_probability_amplifier_current = 1

    # Load model, set this to the model generated by train.py
    model = keras.models.load_model(model_file, custom_objects={'CustomTransformer': CustomTransformer})

    class NodePrediction:
        def __init__(self, crop_trace: pd.DataFrame, probability_of=0):
            self.cropped_trace = crop_trace
            self.cropped_line = ''.join(crop_trace[log_data.act_name_key].tolist())
            self.model_input = encode(self.cropped_line, maxlen, char_indices)
            self.probability_of = probability_of

        def __str__(self):
            return f"Prefix: {self.cropped_line}, prob. {self.probability_of}"

        def __lt__(self, other):
            return -self.probability_of < -other.probability_of

    def jh(trace, prefix_size, log_data, predict_size, pn_file, pt_file, target_indices_char, target_char_indices, thre):

        if len(trace) >= prefix_size:          
            
            trace_name = trace[log_data.case_name_key].tolist()[0]
            trace_prefix = trace.head(prefix_size)
            trace_ground_truth = trace.tail(trace.shape[0] - prefix_size)
            act_ground_truth = ''.join(trace_ground_truth[log_data.act_name_key].tolist())
            act_ground_truth_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in act_ground_truth]
            # Initialize queue for beam search, put root of the tree inside
            visited_nodes: PriorityQueue[NodePrediction] = PriorityQueue()
            visited_nodes.put(NodePrediction(trace_prefix))

            frontier_nodes: PriorityQueue[NodePrediction] = PriorityQueue()
            found_satisfying_constraint = False

            child_node = None
            for i in range(predict_size - prefix_size + 1):
                if len( [i for i in act_ground_truth_org if i in ['Unexpected2', 'Repairing2', 'Unexpected1', 'Repairing1'] ] ) > 0:
                    print("prefix_step: ", i)
                if visited_nodes.empty():
                        break
                for k in range(min(shared.beam_size, len(visited_nodes.queue))):    
                    child_node = visited_nodes.get()
                    if child_node.cropped_line[-1] == "!":
                        # break
                        if k == 0:
                            visited_nodes = PriorityQueue()
                            break
                        else:
                            continue
                    # print("prefix_step = ", i, " , beam_step = ", k)  
                    
                    enc = child_node.model_input
                    temp_cropped_trace = child_node.cropped_trace
                    temp_cropped_line = child_node.cropped_line
                    y = model.predict(enc, verbose=0)  # make predictions
                    
                    # split predictions into separate activity and outcome predictions
                    y_char = y[0]

                    score = y_char
                                     
                    # put top 3 based on score
                    frontier_nodes = get_beam_size(frontier_nodes, NodePrediction, child_node, 
                                                   temp_cropped_line, temp_cropped_trace, score, y_char, act_ground_truth_org,
                                                   target_indices_char, target_char_indices, 
                                                   log_data, beam_size = shared.beam_size)
                    
                    # import pdb
                    # pdb.set_trace()
                visited_nodes = frontier_nodes
                frontier_nodes = PriorityQueue()

            predicted = child_node.cropped_line[(prefix_size):-1]
            # print(child_node.cropped_line)
            predicted_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in predicted]
            act_ground_truth_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in act_ground_truth]
            print("prediction: ", predicted_org)
            print("groundtruth: ", act_ground_truth_org)

            
            output = []
            if len(act_ground_truth) > 0:
                output.append(prefix_size)
                output.append(act_ground_truth)
                output.append(predicted)

                dls = 1 - \
                    (damerau_levenshtein_distance(predicted, act_ground_truth) / max(len(predicted), len(act_ground_truth)))
                if dls < 0:
                    dls = 0
                output.append(dls)
                output.append(1 - distance.jaccard(predicted, act_ground_truth))


                output.append(thre)
                # output.append('1' if compliantness else '0')

            if output:
                with open(output_file, 'a', encoding='utf-8', newline='') as csvfile:
                    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
                    spamwriter.writerow(output)

##############################################################

    with open(output_file, 'w', encoding='utf-8', newline='') as csvfile:
        spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
        # Headers for the new file
        spamwriter.writerow(["Prefix length", "Ground truth", "Predicted", "Damerau-Levenshtein", "Jaccard", "threshold"])


    for prefix_size in range(log_data.evaluation_prefix_start, log_data.evaluation_prefix_end+1):
        compliant_traces = compliant_traces.reset_index(drop=True) 

        for thre in [0, 0.25, 0.5, 0.75, 0.9]: # [0.92, 0.94, 0.96, 0.98, 1.0]:
            tqdm.pandas()
            compliant_traces.groupby(log_data.case_name_key).progress_apply(lambda x: jh(x, prefix_size, log_data,
                                                                                    predict_size, pn_file, pt_file,
                                                                                    target_indices_char,
                                                                                    target_char_indices, thre))
