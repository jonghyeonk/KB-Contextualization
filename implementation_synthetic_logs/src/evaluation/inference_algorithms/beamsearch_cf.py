"""
this file is build based on the code found in evaluate_suffix_and_remaining_time.py
here the beam search (with breath-first-search) is implemented, to find compliant prediction
Author: Anton Yeshchenko
"""

from __future__ import division
import csv
from pathlib import Path
from queue import PriorityQueue
import distance
import numpy as np
import pandas as pd
from tensorflow import keras
from jellyfish import damerau_levenshtein_distance

from src.commons import shared_variables as shared
from src.commons.log_utils import LogData
from src.evaluation.prepare_data import get_beam_size, encode, get_pn_fitness
from src.training.train_common import CustomTransformer
import absl.logging
absl.logging.set_verbosity(absl.logging.ERROR)
from tqdm import tqdm


def run_experiments(log_data: LogData, compliant_traces: pd.DataFrame, maxlen, predict_size, char_indices,
                    target_char_indices, target_indices_char, target_char_indices2, target_indices_char2, char_indices_group, target_char_indices_group,
                    target_indices_char_group, model_file: Path, output_file: Path, 
                    bk_file: Path, method_fitness: str, resource: bool, outcome: bool, weight: list):
    # Find cycles and modify the probability functionality goes here
    stop_symbol_probability_amplifier_current = 1

    # Load model, set this to the model generated by train.py
    model = keras.models.load_model(model_file, custom_objects={'CustomTransformer': CustomTransformer})
    class NodePrediction:
        def __init__(self, crop_trace: pd.DataFrame, probability_of=0):
            self.cropped_trace = crop_trace
            self.cropped_line = ''.join(crop_trace[log_data.act_name_key].tolist())
            if resource:
                self.cropped_line_group = ''.join(crop_trace[log_data.res_name_key].tolist())
            self.model_input = encode(crop_trace, log_data, maxlen, char_indices, char_indices_group, resource)
            self.probability_of = probability_of

        def __str__(self):
            return f"Prefix: {self.cropped_line}, prob. {self.probability_of}"

        def __lt__(self, other):
            return -self.probability_of < -other.probability_of



    class CacheFitness:
        def __init__(self):
            self.trace = {}
            
        def add(self, crop_trace:str, fitness: float):
            self.trace[crop_trace] = fitness
            
        def get(self, crop_trace:str):
            if crop_trace not in self.trace.keys():
                return None
            else:
                return self.trace[crop_trace]


    class CacheTrace:
        def __init__(self):
            self.trace = {}
            
        def add(self, crop_trace:str, output: list):
            self.trace[crop_trace] = output
            
        def get(self, crop_trace:str):
            if crop_trace not in self.trace.keys():
                return None
            else:
                return self.trace[crop_trace]



    def apply_trace(trace, prefix_size, log_data, predict_size, bk_file, target_indices_char, target_char_indices, target_indices_char2, target_char_indices2,
                    target_indices_char_group, target_char_indices_group, method_fitness, resource, outcome, weight):

        if len(trace) > prefix_size:          
            
            trace_name = trace[log_data.case_name_key].iloc[0]
            trace_prefix = trace.head(prefix_size)
            
            act_prefix = ''.join(trace_prefix[log_data.act_name_key].tolist()) + "_" + str(weight)
            
            check_prefix = cache_trace.get(act_prefix)
            if check_prefix == None:

                trace_ground_truth = trace.tail(trace.shape[0] - prefix_size)
                act_ground_truth = ''.join(trace_ground_truth[log_data.act_name_key].tolist())
                act_ground_truth_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in act_ground_truth]
                
                if resource:
                    res_ground_truth = ''.join(trace_ground_truth[log_data.res_name_key].tolist())
                if outcome:
                    outcome_ground_truth = trace[log_data.label_name_key].iloc[0]

                # Initialize queue for beam search, put root of the tree inside
                visited_nodes: PriorityQueue[NodePrediction] = PriorityQueue()
                visited_nodes.put(NodePrediction(trace_prefix))

                frontier_nodes: PriorityQueue[NodePrediction] = PriorityQueue()
                found_satisfying_constraint = False

                child_node = None
                record_update = []
                for i in range(predict_size - prefix_size + 1):
                    # if len( [i for i in act_ground_truth_org if i in ['Unexpected2', 'Repairing2', 'Unexpected1', 'Repairing1'] ] ) > 0:
                    # print("prefix_step: ", i)
                    if visited_nodes.empty():
                            break
                    
                    for k in range(min(shared.beam_size, len(visited_nodes.queue))):                        
                        child_node = visited_nodes.get()

                        if child_node.cropped_line[-1] == "!":
                            if k == 0:
                                visited_nodes = PriorityQueue()
                                break
                            else:
                                continue


                        # if child_node.cropped_line[-1] in log_data.new_chars:
                        #     1
                            
                        enc = child_node.model_input
                        temp_cropped_trace = child_node.cropped_trace
                        temp_cropped_line = child_node.cropped_line
                        
                        y = model.predict(enc, verbose=0)  # make predictions
                        # split predictions into separate activity and outcome predictions
                        
                        if ~resource and ~outcome:
                            y_char = y[0]
                            y_group = None
                            y_o = None
                        elif ~resource and outcome:
                            y_char = y[0][0]
                            y_group = None
                            y_o = y[1][0][0]
                        elif resource and outcome:
                            y_char = y[0][0]
                            y_group = y[1][0]
                            y_o = y[2][0][0]
                        
                        # combine with fitness
                        if method_fitness is None or weight == 0:
                            fitness = []
                            fitness_temp = []
                            score = y_char
                            
                        else:
                            fitness = [] 
                            fitness_temp = []
                            
                            for f in range(len(target_indices_char) + len(log_data.new_chars)):
                                
                                if f < len(target_indices_char):
                                    temp_prediction = target_indices_char[f]               
                                else:
                                    temp_prediction = log_data.new_chars[f-len(target_indices_char)]
                                    
                                predicted_row = temp_cropped_trace.tail(1).copy()
                                predicted_row.loc[:, log_data.act_name_key] = temp_prediction
                                temp_cropped_trace_next= pd.concat([temp_cropped_trace, predicted_row])                        
                                temp_cropped_line_next = ''.join(temp_cropped_trace_next[log_data.act_name_key].tolist()) 
                                                                
                                check_cache = cache_fitness.get(temp_cropped_line_next )
                                if check_cache == None:
                                    fitness_current = get_pn_fitness(bk_file, method_fitness, temp_cropped_trace_next, log_data)[trace_name]
                                    cache_fitness.add(temp_cropped_line_next, fitness_current)
                                else:
                                    fitness_current = check_cache
                                    
                                fitness = fitness +  [np.exp(fitness_current) ]
                                fitness_temp = fitness_temp +  [fitness_current]
                                                            
                            if sum(fitness) > 0:
                                fitness = [f/sum(fitness) for f in fitness] 
                            else:
                                fitness = np.repeat(1/len(fitness),len(fitness)).tolist()
                                fitness_temp = np.repeat(1/len(fitness_temp),len(fitness_temp)).tolist()
                                
                            if len(log_data.new_chars) > 0:
                                y_char = y_char + min(y_char)*len(log_data.new_chars)
                            score = [pow(a,1-weight)*pow(b,weight) for a,b in zip(y_char, fitness)]
                                        
                        # put top 3 based on score
                        frontier_nodes, record = get_beam_size(frontier_nodes, NodePrediction, child_node, 
                                                    temp_cropped_line, temp_cropped_trace, score, y_group, y_char, fitness_temp, act_ground_truth_org,
                                                    char_indices, target_indices_char2, target_char_indices2, 
                                                    target_indices_char_group, target_char_indices_group, i,
                                                    log_data, resource, beam_size = shared.beam_size)
                        
                        record_update = record_update + record
                        
                    visited_nodes = frontier_nodes
                    frontier_nodes = PriorityQueue()

                predicted = child_node.cropped_line[prefix_size:-1]
                if resource:
                    predicted_group = child_node.cropped_line_group[prefix_size:-1]
                
                if outcome:
                    predicted_outcome = '1' if y_o >= 0.5 else '0'

                # predicted_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in predicted]
                act_ground_truth_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in act_ground_truth]
                # print("prediction: ", predicted_org)
                # print("groundtruth: ", act_ground_truth_org)
                
                output = []
                if len(act_ground_truth) > 0:
                    output.append(trace_name)

                        
                    output.append(log_data.case_to_variant[trace_name])
                    output.append( list(log_data.case_to_variant.values()).count(log_data.case_to_variant[trace_name])  )
                    output.append( '-'  )
                    output.append( list(log_data.case_to_variant_test.values()).count(log_data.case_to_variant_test[trace_name])  )
                    output.append(prefix_size)
                    output.append(act_ground_truth)
                    output.append(predicted)

                    dls = 1 - \
                        (damerau_levenshtein_distance(predicted, act_ground_truth) / max(len(predicted), len(act_ground_truth)))
                    if dls < 0:
                        dls = 0
                    output.append(dls)
                    output.append(1 - distance.jaccard(predicted, act_ground_truth))
                    
                    if resource:
                        output.append(res_ground_truth)
                        output.append(predicted_group)
                        dls_res = 1 - \
                            (damerau_levenshtein_distance(predicted_group, res_ground_truth)
                                / max(len(predicted_group), len(res_ground_truth)))
                        if dls_res < 0:
                            dls_res = 0
                        output.append(dls_res)
                    
                    if outcome:
                        output.append(outcome_ground_truth)
                        output.append(predicted_outcome)
                        output.append('1' if outcome_ground_truth == predicted_outcome else '0')

                    output.append(weight)
                    output.append(' '.join(record_update))
                    output.append('>>'.join(act_ground_truth_org))
                    
                    prediction_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in predicted]
                    output.append('>>'.join(prediction_org))
                    
                    # output.append('1' if compliantness else '0')
                    cache_trace.add(act_prefix, output)
                    
            else:
                trace_ground_truth = trace.tail(trace.shape[0] - prefix_size)
                act_ground_truth = ''.join(trace_ground_truth[log_data.act_name_key].tolist())
                output = []
                
                output.append(trace_name)
                output.append(log_data.case_to_variant[trace_name])
                output.append( list(log_data.case_to_variant.values()).count(log_data.case_to_variant[trace_name])  )
                output.append( '-'  )
                output.append( list(log_data.case_to_variant_test.values()).count(log_data.case_to_variant_test[trace_name])  )
                output.append(prefix_size)
                output.append(act_ground_truth)
                
                predicted = check_prefix[7]
                output.append(predicted)
                
                dls = 1 - \
                    (damerau_levenshtein_distance(predicted, act_ground_truth) / max(len(predicted), len(act_ground_truth)))
                if dls < 0:
                    dls = 0
                output.append(dls)
                output.append(1 - distance.jaccard(predicted, act_ground_truth))
                output = output + check_prefix[10:(len(check_prefix)-2)]
                
                act_ground_truth_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in act_ground_truth]
                output.append('>>'.join(act_ground_truth_org))
                
                prediction_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in predicted]
                output.append('>>'.join(prediction_org))
                

            if output:
                with open(output_file, 'a', encoding='utf-8', newline='') as csvfile:
                    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
                    spamwriter.writerow(output)


##############################################################

    with open(output_file, 'w', encoding='utf-8', newline='') as csvfile:
        spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
        # Headers for the new file
        if ~resource and ~outcome:
            spamwriter.writerow(["Case ID", "Variant ID", "Variant_size", "Variant_size_train", "Variant_size_test", "Prefix length", "Ground truth", "Predicted", "Damerau-Levenshtein", "Jaccard",
                                "Weight",  "Record", "Record_org", "Prediction_org"])
        if ~resource and outcome:
            spamwriter.writerow(["Case ID", "Variant ID", "Variant_size", "Prefix length", "Ground truth", "Predicted", "Damerau-Levenshtein", "Jaccard",
                                "Ground truth outcome", "Predicted outcome", "Outcome diff.", "Weight"])
        elif resource and outcome:
            spamwriter.writerow(["Case ID", "Variant ID", "Variant_size", "Prefix length", "Ground truth", "Predicted", "Damerau-Levenshtein", "Jaccard",
                                 "Ground Truth Group", "Predicted Group", "Damerau-Levenshtein Resource",
                                "Ground truth outcome", "Predicted outcome", "Outcome diff.", "Weight"])
            
    cache_fitness = CacheFitness()
    cache_trace = CacheTrace()
    for prefix_size in range(log_data.evaluation_prefix_start, log_data.evaluation_prefix_end+1):
        compliant_traces = compliant_traces.reset_index(drop=True) 
        for w in weight: 
            tqdm.pandas()
            compliant_traces.groupby(log_data.case_name_key).progress_apply(lambda x: apply_trace(x, prefix_size, log_data,
                                                                                    predict_size, bk_file,
                                                                                    target_indices_char, target_char_indices, 
                                                                                    target_indices_char2, target_char_indices2,
                                                                                    target_indices_char_group, target_char_indices_group,
                                                                                    method_fitness, resource, outcome, w))
