"""
this file is built based on the code found in evaluate_suffix_and_remaining_time.py
here the beam search (with breath-first-search) is implemented, to find compliant prediction
The code is expanded to also consider the Resource attribute
"""

from __future__ import division
import csv
from pathlib import Path
from queue import PriorityQueue
import distance
import numpy as np
import pandas as pd
from tensorflow import keras
from jellyfish import damerau_levenshtein_distance

from src.commons import shared_variables as shared
from src.commons.log_utils import LogData
from src.evaluation.prepare_data import encode_with_group, get_predictions, get_pn_fitness
from src.training.train_common import CustomTransformer

from tqdm import tqdm

def run_experiments(log_data: LogData, compliant_traces: pd.DataFrame, maxlen, predict_size, char_indices,
                    target_char_indices, target_indices_char, char_indices_group, target_char_indices_group,
                    target_indices_char_group, model_file: Path, output_file: Path, pn_file: Path):
    # Find cycles and modify the probability functionality goes here
    stop_symbol_probability_amplifier_current = 1

    # Load model, set this to the model generated by train.py
    model = keras.models.load_model(model_file, custom_objects={'CustomTransformer': CustomTransformer})

    class NodePrediction:
        def __init__(self, crop_trace: pd.DataFrame, probability_of=0):
            self.cropped_trace = crop_trace
            self.cropped_line = ''.join(crop_trace[log_data.act_name_key].tolist())
            self.cropped_line_group = ''.join(crop_trace[log_data.res_name_key].tolist())
            
            self.cropped_line_time = crop_trace[log_data.timestamp_key].tolist()     #jh
            # self.cropped_line_time2 = crop_trace[log_data.timestamp_key2].tolist()     #jh

            self.model_input = encode_with_group(self.cropped_line, self.cropped_line_group,
                                                 self.cropped_line_time,  #jh
                                                   maxlen, char_indices,
                                                 char_indices_group)
            self.probability_of = probability_of

        def __str__(self):
            return f"Prefix: {self.cropped_line}, prob. {self.probability_of}"

        def __lt__(self, other):
            return -self.probability_of < -other.probability_of


    def jh(trace, prefix_size, log_data, predict_size, pn_file, target_indices_char, target_char_indices, target_indices_char_group, target_char_indices_group):

        if trace.shape[0] >= prefix_size:
            trace_prefix = trace.head(prefix_size)
            trace_ground_truth = trace.tail(trace.shape[0] - prefix_size)
            act_ground_truth = ''.join(trace_ground_truth[log_data.act_name_key].tolist())
            res_ground_truth = ''.join(trace_ground_truth[log_data.res_name_key].tolist())

            ground_truth_time = trace[log_data.timestamp_key].tolist()     #jh
            # ground_truth_time2 = trace[log_data.timestamp_key2].tolist()     #jh

            outcome_ground_truth = trace[log_data.label_name_key].iloc[0]

            # Initialize queue for beam search, put root of the tree inside
            queue_next_steps: PriorityQueue[NodePrediction] = PriorityQueue()
            queue_next_steps.put(NodePrediction(trace_prefix))

            queue_next_steps_future: PriorityQueue[NodePrediction] = PriorityQueue()
            found_satisfying_constraint = False

            current_beam_size = shared.beam_size
            current_prediction_premis = None

            for i in range(predict_size - prefix_size):
                for k in range(current_beam_size):
                    if queue_next_steps.empty():
                        break

                    current_prediction_premis = queue_next_steps.get()

                    if not found_satisfying_constraint:
                        # if get_pn_fitness(pn_file, current_prediction_premis.cropped_trace, log_data)[trace_name] \
                        #         >= log_data.evaluation_th:   # JH: DELETED
                            # the trace is verified, and we can just finish the predictions
                            # beam size is 1 because predict only sequence of events
                        current_beam_size = 1
                        current_prediction_premis.probability_of = 0.0
                        # overwrite new queue
                        queue_next_steps_future = PriorityQueue()
                        found_satisfying_constraint = True

                    enc = current_prediction_premis.model_input
                    temp_cropped_trace = current_prediction_premis.cropped_trace
                    temp_cropped_line = current_prediction_premis.cropped_line
                    temp_cropped_line_group = current_prediction_premis.cropped_line_group
                    temp_cropped_line_time = current_prediction_premis.cropped_line_time #jh
                    # temp_cropped_line_time2 = current_prediction_premis.cropped_line_time2 #jh
                    
                    # print(model)
                    y = model.predict(enc, verbose=0)  # make predictions

                    # split predictions into seperate activity, resource and outcome predictions
                    y_char = y[0][0]
                    y_group = y[1][0]
                    y_time = y[2][0]
                    # y_time2 = y[3][0]
                    y_o = y[3][0][0]

                    for j in range(current_beam_size):
                        temp_prediction, temp_prediction_group \
                            = get_predictions(temp_cropped_line, temp_cropped_line_group, y_char, y_group,
                                                target_indices_char, target_char_indices, target_indices_char_group,
                                                target_char_indices_group, ith_best=j)

                        temp_prediction_time = temp_cropped_line_time[j]
                        # temp_prediction_time2 = temp_cropped_line_time2[j]

                        # end of case was just predicted, therefore, stop predicting further into the future
                        if temp_prediction == '!':
                            # if get_pn_fitness(pn_file, temp_cropped_trace, log_data)[trace_name] \ #JH :DELETED
                            #         >= log_data.evaluation_th:
                            #     stop_symbol_probability_amplifier_current = 1
                            #     queue_next_steps = PriorityQueue()
                            #     break
                            # else:
                            #     continue
                            
                            # stop_symbol_probability_amplifier_current = 1
                            queue_next_steps = PriorityQueue()
                            
                            
                        predicted_row = temp_cropped_trace.tail(1).copy()
                        predicted_row.loc[:, log_data.act_name_key] = temp_prediction
                        predicted_row.loc[:, log_data.res_name_key] = temp_prediction_group
                        predicted_row.loc[:, log_data.timestamp_key] = temp_prediction_time
                        # predicted_row.loc[:, log_data.timestamp_key2] = temp_prediction_time2

                        temp_cropped_trace = pd.concat([temp_cropped_trace, predicted_row])

                        probability_this = np.sort(y_char)[len(y_char) - 1 - j]

                        temp = NodePrediction(temp_cropped_trace,
                                                current_prediction_premis.probability_of + np.log(probability_this))

                        queue_next_steps_future.put(temp)

                queue_next_steps = queue_next_steps_future
                queue_next_steps_future = PriorityQueue()

            if current_prediction_premis is not None:
                predicted = current_prediction_premis.cropped_line[prefix_size:]
                predicted_group = current_prediction_premis.cropped_line_group[prefix_size:]
                predicted_time = current_prediction_premis.cropped_line_time[prefix_size:]
                # predicted_time2 = current_prediction_premis.cropped_line_time2[prefix_size:]
                predicted_outcome = '1' if y_o >= 0.5 else '0'


                output = []
                if len(act_ground_truth) > 0:
                    output.append(prefix_size)
                    output.append(act_ground_truth)
                    output.append(predicted)
                    dls = 1 - \
                        (damerau_levenshtein_distance(predicted, act_ground_truth) / max(len(predicted), len(act_ground_truth)))
                    if dls < 0:
                        dls = 0
                    output.append(dls)
                    output.append(1 - distance.jaccard(predicted, act_ground_truth))

                    output.append(res_ground_truth)
                    output.append(predicted_group)
                    dls_res = 1 - \
                        (damerau_levenshtein_distance(predicted_group, res_ground_truth)
                            / max(len(predicted_group), len(res_ground_truth)))
                    if dls_res < 0:
                        dls_res = 0
                    output.append(dls_res)

                    output.append(ground_truth_time )
                    output.append(predicted_time )
                    
                    # output.append(ground_truth_time2)
                    # output.append(predicted_time2)  

                    output.append(outcome_ground_truth)
                    output.append(predicted_outcome)
                    output.append('1' if outcome_ground_truth == predicted_outcome else '0')


                if output:
                    with open(output_file, 'a',encoding='utf-8', newline='') as csvfile:
                        spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
                        spamwriter.writerow(output)     

    with open(output_file, 'w', encoding='utf-8', newline='') as csvfile:
        spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
        # Headers for the new file
        spamwriter.writerow(["Prefix length", "Ground truth", "Predicted", "Damerau-Levenshtein", "Jaccard",
                             "Ground Truth Group", "Predicted Group", "Damerau-Levenshtein Resource",
                             "Ground truth time", "Predicted time", 
                             "Ground truth outcome", "Predicted outcome", "Outcome diff."])



    for prefix_size in range(log_data.evaluation_prefix_start, log_data.evaluation_prefix_end+1):
        print("Prefix size: " + str(prefix_size))
        
        # if __name__ == '__main__':
            # client = Client(n_workers=8, threads_per_worker=4, processes=True, memory_limit='4GB')

        print(compliant_traces.head())
        # ddf = dd.from_pandas(compliant_traces, npartitions=8)
        
        tqdm.pandas()
        compliant_traces.groupby(log_data.case_name_key).progress_apply(lambda x: jh(x, prefix_size, log_data,
                                                                                predict_size, pn_file, 
                                                                                target_indices_char,
                                                                                target_char_indices, 
                                                                                target_indices_char_group,
                                                                                target_char_indices_group))
        
