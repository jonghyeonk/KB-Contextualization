"""
this file is build based on the code found in evaluate_suffix_and_remaining_time.py
here the beam search (with breath-first-search) is implemented, to find compliant prediction
Author: Anton Yeshchenko
"""

from __future__ import division
import csv
from pathlib import Path
from queue import PriorityQueue
import distance
import numpy as np
import pandas as pd
from tensorflow import keras
from jellyfish import damerau_levenshtein_distance

from src.commons import shared_variables as shared
from src.commons.log_utils import LogData
from src.evaluation.prepare_data import get_beam_size, encode, get_pn_fitness
from src.training.train_common import CustomTransformer
import absl.logging
absl.logging.set_verbosity(absl.logging.ERROR)
from tqdm import tqdm


def run_experiments(log_data: LogData, compliant_traces: pd.DataFrame, maxlen, predict_size, char_indices,
                    target_char_indices, target_indices_char, char_indices_group, target_char_indices_group,
                    target_indices_char_group, model_file: Path, output_file: Path, 
                    bk_file: Path, method_fitness: str, resource: bool, outcome: bool, weight: list):
    # Find cycles and modify the probability functionality goes here
    stop_symbol_probability_amplifier_current = 1

    # Load model, set this to the model generated by train.py
    model = keras.models.load_model(model_file, custom_objects={'CustomTransformer': CustomTransformer})
    class NodePrediction:
        def __init__(self, crop_trace: pd.DataFrame, probability_of=0):
            self.cropped_trace = crop_trace
            self.cropped_line = ''.join(crop_trace[log_data.act_name_key].tolist())
            if resource:
                self.cropped_line_group = ''.join(crop_trace[log_data.res_name_key].tolist())
            self.model_input = encode(crop_trace, log_data, maxlen, char_indices, char_indices_group, resource)
            self.probability_of = probability_of

        def __str__(self):
            return f"Prefix: {self.cropped_line}, prob. {self.probability_of}"

        def __lt__(self, other):
            return -self.probability_of < -other.probability_of



    class CacheFitness:
        def __init__(self):
            self.trace = {}
            
        def add(self, crop_trace:str, fitness: float):
            self.trace[crop_trace] = fitness
            
        def get(self, crop_trace:str):
            if crop_trace not in self.trace.keys():
                return None
            else:
                return self.trace[crop_trace]



    def apply_trace(trace, prefix_size, log_data, predict_size, bk_file, target_indices_char, target_char_indices, 
                    target_indices_char_group, target_char_indices_group, method_fitness, resource, outcome, weight):

        if len(trace) >= prefix_size:          
            
            trace_name = trace[log_data.case_name_key].iloc[0]
            trace_prefix = trace.head(prefix_size)
            trace_ground_truth = trace.tail(trace.shape[0] - prefix_size)
            act_ground_truth = ''.join(trace_ground_truth[log_data.act_name_key].tolist())
            act_ground_truth_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in act_ground_truth]
            if resource:
                res_ground_truth = ''.join(trace_ground_truth[log_data.res_name_key].tolist())
            if outcome:
                outcome_ground_truth = trace[log_data.label_name_key].iloc[0]

            # Initialize queue for beam search, put root of the tree inside
            visited_nodes: PriorityQueue[NodePrediction] = PriorityQueue()
            visited_nodes.put(NodePrediction(trace_prefix))

            frontier_nodes: PriorityQueue[NodePrediction] = PriorityQueue()
            found_satisfying_constraint = False

            child_node = None
            record_update = []
            for i in range(predict_size - prefix_size + 1):
                # if len( [i for i in act_ground_truth_org if i in ['Unexpected2', 'Repairing2', 'Unexpected1', 'Repairing1'] ] ) > 0:
                # print("prefix_step: ", i)
                if visited_nodes.empty():
                        break
                
                for k in range(min(shared.beam_size, len(visited_nodes.queue))):                        
                    child_node = visited_nodes.get()

                    if child_node.cropped_line[-1] == "!":
                        if k == 0:
                            visited_nodes = PriorityQueue()
                            break
                        else:
                            continue

                    enc = child_node.model_input
                    temp_cropped_trace = child_node.cropped_trace
                    temp_cropped_line = child_node.cropped_line
                    
                    y = model.predict(enc, verbose=0)  # make predictions
                    # split predictions into separate activity and outcome predictions
                    
                    if ~resource and ~outcome:
                        y_char = y[0]
                        y_group = None
                        y_o = None
                    elif ~resource and outcome:
                        y_char = y[0][0]
                        y_group = None
                        y_o = y[1][0][0]
                    elif resource and outcome:
                        y_char = y[0][0]
                        y_group = y[1][0]
                        y_o = y[2][0][0]
                    
                    # combine with fitness
                    if method_fitness is None or weight == 0:
                        fitness = []
                        score = y_char
                        
                    else:
                        fitness = [] 
                        for f in range(len(y_char)):
                            temp_prediction = target_indices_char[f]
                            predicted_row = temp_cropped_trace.tail(1).copy()
                            predicted_row.loc[:, log_data.act_name_key] = temp_prediction
                            temp_cropped_trace_next= pd.concat([temp_cropped_trace, predicted_row])                        
                            temp_cropped_line_next = ''.join(temp_cropped_trace_next[log_data.act_name_key].tolist())                 
                            
                            check_cache = cache_fitness.get(temp_cropped_line_next)
                            if check_cache == None:
                                fitness_current = get_pn_fitness(bk_file, method_fitness, temp_cropped_trace_next, log_data)[trace_name]
                                cache_fitness.add(temp_cropped_line_next, fitness_current)
                            else:
                                fitness_current = check_cache
                                
                            fitness = fitness +  [fitness_current ]
                                                        
                        if sum(fitness) > 0:
                            fitness = [f/sum(fitness) for f in fitness] 
                        else:
                            fitness = np.repeat(1/len(fitness),len(fitness)).tolist()
                            
                        score = [pow(a,1-weight)*pow(b,weight) for a,b in zip(y_char, fitness)]
                                     
                    # put top 3 based on score
                    frontier_nodes, record = get_beam_size(frontier_nodes, NodePrediction, child_node, 
                                                   temp_cropped_line, temp_cropped_trace, score, y_group, y_char, fitness, act_ground_truth_org,
                                                   char_indices, target_indices_char, target_char_indices, 
                                                   target_indices_char_group, target_char_indices_group, i,
                                                   log_data, resource, beam_size = shared.beam_size)
                    
                    record_update = record_update + record
                    
                visited_nodes = frontier_nodes
                frontier_nodes = PriorityQueue()

            predicted = child_node.cropped_line[prefix_size:-1]
            if resource:
                predicted_group = child_node.cropped_line_group[prefix_size:-1]
            
            if outcome:
                predicted_outcome = '1' if y_o >= 0.5 else '0'

            # predicted_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in predicted]
            act_ground_truth_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in act_ground_truth]
            # print("prediction: ", predicted_org)
            # print("groundtruth: ", act_ground_truth_org)
            
            output = []
            if len(act_ground_truth) > 0:
                output.append(trace_name)

                    
                output.append(log_data.case_to_variant[trace_name])
                output.append( list(log_data.case_to_variant.values()).count(log_data.case_to_variant[trace_name])  )

                output.append(prefix_size)
                output.append(act_ground_truth)
                output.append(predicted)

                dls = 1 - \
                    (damerau_levenshtein_distance(predicted, act_ground_truth) / max(len(predicted), len(act_ground_truth)))
                if dls < 0:
                    dls = 0
                output.append(dls)
                output.append(1 - distance.jaccard(predicted, act_ground_truth))
                
                if resource:
                    output.append(res_ground_truth)
                    output.append(predicted_group)
                    dls_res = 1 - \
                        (damerau_levenshtein_distance(predicted_group, res_ground_truth)
                            / max(len(predicted_group), len(res_ground_truth)))
                    if dls_res < 0:
                        dls_res = 0
                    output.append(dls_res)
                
                if outcome:
                    output.append(outcome_ground_truth)
                    output.append(predicted_outcome)
                    output.append('1' if outcome_ground_truth == predicted_outcome else '0')

                output.append(weight)
                output.append(' '.join(record_update))
                output.append('>>'.join(act_ground_truth_org))
                # output.append('1' if compliantness else '0')

            if output:
                with open(output_file, 'a', encoding='utf-8', newline='') as csvfile:
                    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
                    spamwriter.writerow(output)


##############################################################

    with open(output_file, 'w', encoding='utf-8', newline='') as csvfile:
        spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
        # Headers for the new file
        if ~resource and ~outcome:
            spamwriter.writerow(["Case ID", "Variant ID", "Variant_size", "Prefix length", "Ground truth", "Predicted", "Damerau-Levenshtein", "Jaccard",
                                "Weight", "Record", "Record_org"])
        if ~resource and outcome:
            spamwriter.writerow(["Case ID", "Variant ID", "Variant_size", "Prefix length", "Ground truth", "Predicted", "Damerau-Levenshtein", "Jaccard",
                                "Ground truth outcome", "Predicted outcome", "Outcome diff.", "Weight"])
        elif resource and outcome:
            spamwriter.writerow(["Case ID", "Variant ID", "Variant_size", "Prefix length", "Ground truth", "Predicted", "Damerau-Levenshtein", "Jaccard",
                                 "Ground Truth Group", "Predicted Group", "Damerau-Levenshtein Resource",
                                "Ground truth outcome", "Predicted outcome", "Outcome diff.", "Weight"])
            
    cache_fitness = CacheFitness()
    for prefix_size in range(log_data.evaluation_prefix_start, log_data.evaluation_prefix_end+1):
        compliant_traces = compliant_traces.reset_index(drop=True) 
        for w in weight: 
            tqdm.pandas()
            compliant_traces.groupby(log_data.case_name_key).progress_apply(lambda x: apply_trace(x, prefix_size, log_data,
                                                                                    predict_size, bk_file,
                                                                                    target_indices_char, target_char_indices, 
                                                                                    target_indices_char_group, target_char_indices_group,
                                                                                    method_fitness, resource, outcome, w))
